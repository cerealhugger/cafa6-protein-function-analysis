{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98bd2d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (5000, 322)\n",
      "X shape: (5000, 320)\n",
      "Y shape: (5000, 8910)\n",
      "Train shape: (4000, 320) (4000, 8910)\n",
      "Val shape: (1000, 320) (1000, 8910)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "merged = pd.read_parquet(\"train_demo.parquet\")\n",
    "print(\"Merged shape:\", merged.shape)\n",
    "\n",
    "X = merged.drop(columns=[\"protein_id\", \"term\"]).astype(float).values\n",
    "Y_raw = merged[\"term\"]\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import joblib\n",
    "\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "Y = mlb.fit_transform(Y_raw)\n",
    "joblib.dump(mlb, \"mlb.pkl\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train (80%) and validation (20%)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, Y_train.shape)\n",
    "print(\"Val shape:\", X_val.shape, Y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "490975fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train loss: 0.2025\n",
      "   Val loss: 0.0114 | F1: 0.0000 | Elem acc: 0.9993\n",
      "Epoch 2 Train loss: 0.0113\n",
      "   Val loss: 0.0098 | F1: 0.0000 | Elem acc: 0.9993\n",
      "Epoch 3 Train loss: 0.0086\n",
      "   Val loss: 0.0076 | F1: 0.0000 | Elem acc: 0.9993\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----- Sparse Dataset -----\n",
    "class SparseDataset(Dataset):\n",
    "    def __init__(self, X, Y_sparse):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.Y = Y_sparse.tocsr()   # fast row access\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = torch.tensor(self.Y[idx].toarray(), dtype=torch.float32).squeeze(0)\n",
    "        return self.X[idx], y\n",
    "\n",
    "# ----- Model -----\n",
    "class GOClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden=512, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden // 2, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ----- Setup -----\n",
    "model = GOClassifier(input_dim=X.shape[1],\n",
    "    output_dim=Y.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_dataset = SparseDataset(X_train, Y_train)\n",
    "val_dataset   = SparseDataset(X_val,   Y_val)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader    = DataLoader(val_dataset,   batch_size=256, shuffle=False)\n",
    "\n",
    "# ----- Training + Validation -----\n",
    "best_f1 = 0.0\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Train loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # --- Validation (batched) ---\n",
    "    model.eval()\n",
    "    val_loss, preds_list, true_list = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            val_loss += criterion(out, yb).item()\n",
    "            preds_list.append((out > 0.5).cpu().numpy())\n",
    "            true_list.append(yb.cpu().numpy())\n",
    "\n",
    "    preds = np.vstack(preds_list)\n",
    "    trues = np.vstack(true_list)\n",
    "    f1 = f1_score(trues, preds, average='micro')\n",
    "    elementwise_acc = (preds == trues).mean()\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        torch.save(model.state_dict(), \"best_go_classifier.pth\")\n",
    "        print(f\"   ✅ New best model saved (F1={best_f1:.4f})\")\n",
    "\n",
    "    print(f\"   Val loss: {val_loss/len(val_loader):.4f} | F1: {f1:.4f} | Elem acc: {elementwise_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69592b34",
   "metadata": {},
   "source": [
    "| Hyperparameter    | Why it matters                |\n",
    "| ----------------- | ----------------------------- |\n",
    "| **hidden_dim**    | controls model capacity       |\n",
    "| **dropout**       | prevents overfitting          |\n",
    "| **learning rate** | biggest effect on convergence |\n",
    "| **batch size**    | stability of gradients        |\n",
    "| **activation**    | ReLU vs GELU                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fdc520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb_df = pd.read_parquet(\"test_demo.parquet\")\n",
    "\n",
    "def extract_uniprot_id(pid):\n",
    "    parts = pid.split('|')\n",
    "    return parts[1] if len(parts) >= 2 else pid\n",
    "\n",
    "test_emb_df[\"protein_id\"] = test_emb_df[\"protein_id\"].apply(extract_uniprot_id)\n",
    "X_test = test_emb_df.drop(columns=[\"protein_id\"]).astype(float).values\n",
    "protein_ids = test_emb_df[\"protein_id\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f67684e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions and saving to submission.tsv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 4/4 [00:06<00:00,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Submission saved to submission.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "# ======== CONFIG ========\n",
    "batch_size = 256        # adjust based on GPU memory\n",
    "threshold = 0.01        # only save scores above this (saves disk space)\n",
    "outfile = \"submission.tsv\"\n",
    "go_terms = mlb.classes_ # GO term order from training\n",
    "\n",
    "# ======== Predict + Write ========\n",
    "print(f\"Generating predictions and saving to {outfile} ...\")\n",
    "\n",
    "with open(outfile, \"w\") as f_out:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in trange(0, len(X_test), batch_size, desc=\"Predicting\"):\n",
    "            xb = torch.tensor(X_test[i:i+batch_size], dtype=torch.float32).to(device)\n",
    "            probs = model(xb).cpu().numpy()          # shape: [batch, 26125]\n",
    "            ids_batch = protein_ids[i:i+batch_size]  # UniProt IDs\n",
    "\n",
    "            # Stream output to file\n",
    "            for pid, row in zip(ids_batch, probs):\n",
    "                high = row > threshold\n",
    "                for go, p in zip(go_terms[high], row[high]):\n",
    "                    f_out.write(f\"{pid}\\t{go}\\t{p:.3f}\\n\")\n",
    "\n",
    "print(f\"\\n✅ Submission saved to {outfile}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
