{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAFA6 Protein Function Prediction - CNN Head over frozen ESM2\n",
    "\n",
    "This notebook trains a CNN head over a frozen ESM2 backbone for protein function prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# Original absolute path (commented out):\n",
    "# BASE_PATH = \"/Users/zhouheng/Desktop/kaggle/cafa-6-protein-function-prediction\"\n",
    "# SUBMISSION_PATH = \"/Users/zhouheng/Desktop/kaggle/submission_cnn.tsv\"\n",
    "\n",
    "# New relative paths:\n",
    "BASE_PATH = \".\"\n",
    "MODEL_NAME = \"facebook/esm2_t48_15B_UR50D\"  # HuggingFace model id for ESM2\n",
    "# MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"  # Smaller model for testing\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 8\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MAX_LENGTH = 1022\n",
    "TOP_K = 50\n",
    "THRESHOLD = 0.5\n",
    "SUBMISSION_PATH = \"./submission_cnn.tsv\"\n",
    "PREDICT_SPLIT = \"trainval\"\n",
    "SAMPLE_N = 0\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def pick_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return \"mps\"  # Apple Metal\n",
    "    return \"cpu\"\n",
    "\n",
    "def fail(msg: str):\n",
    "    print(f\"[ERROR] {msg}\", file=sys.stderr)\n",
    "    raise RuntimeError(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_glob(root: Path, pattern: str) -> Path:\n",
    "    hits = list(root.rglob(pattern))\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "def load_tables(base: Path) -> Tuple[pd.DataFrame, pd.DataFrame, Path, Path]:\n",
    "    train_path = None\n",
    "    for cand in [\"Train\", \"train\", \"Training\", \"training\"]:\n",
    "        p = base / cand\n",
    "        if p.exists():\n",
    "            train_path = p\n",
    "            break\n",
    "    if train_path is None:\n",
    "        hit = robust_glob(base, \"train_terms.tsv\")\n",
    "        if hit: train_path = hit.parent\n",
    "    if train_path is None:\n",
    "        fail(\"Could not locate Train folder or train_terms.tsv\")\n",
    "\n",
    "    test_path = None\n",
    "    for cand in [\"Test\", \"test\", \"PublicTest\", \"testing\"]:\n",
    "        p = base / cand\n",
    "        if p.exists():\n",
    "            test_path = p\n",
    "            break\n",
    "    if test_path is None:\n",
    "        hit = robust_glob(base, \"test*superset*.fasta*\")\n",
    "        if hit: test_path = hit.parent\n",
    "\n",
    "    tt = pd.read_csv(train_path/\"train_terms.tsv\", sep=\"\\t\",\n",
    "                     header=None, names=[\"protein_id\", \"go_term\", \"ontology\"])\n",
    "    tax = pd.read_csv(train_path/\"train_taxonomy.tsv\", sep=\"\\t\",\n",
    "                      header=None, names=[\"protein_id\", \"taxon_id\"])\n",
    "\n",
    "    train_fasta = train_path/\"train_sequences.fasta\"\n",
    "    test_fasta = None\n",
    "    if test_path:\n",
    "        cand = test_path/\"testsuperset.fasta\"\n",
    "        if cand.exists():\n",
    "            test_fasta = cand\n",
    "        else:\n",
    "            hit = robust_glob(test_path, \"test*superset*.fasta*\")\n",
    "            test_fasta = hit if hit else None\n",
    "\n",
    "    return tt, tax, train_fasta, test_fasta\n",
    "\n",
    "def parse_protein_id(record_id: str) -> str:\n",
    "    rid = record_id.split()[0]\n",
    "    if \"|\" in rid:\n",
    "        parts = rid.split(\"|\")\n",
    "        return parts[1] if len(parts) > 1 and parts[1] else parts[0]\n",
    "    return rid\n",
    "\n",
    "def load_fasta_dict(fasta_path: Path) -> Dict[str, str]:\n",
    "    try:\n",
    "        from Bio import SeqIO\n",
    "    except Exception:\n",
    "        fail(\"Missing dependency: biopython. Please `pip install biopython`.\")\n",
    "    seqs = {}\n",
    "    for rec in SeqIO.parse(str(fasta_path), \"fasta\"):\n",
    "        pid = parse_protein_id(str(rec.id))\n",
    "        seqs[pid] = str(rec.seq)\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pid_list: List[str], y_tensor: torch.Tensor, seq_dict: Dict[str, str]):\n",
    "        self.pids = pid_list\n",
    "        self.y = y_tensor\n",
    "        self.seq = seq_dict\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        pid = self.pids[i]\n",
    "        return pid, self.seq[pid], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, num_labels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, 256, kernel_size=9, padding=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(256, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(256, num_labels)\n",
    "    \n",
    "    def forward(self, token_feats: torch.Tensor) -> torch.Tensor:\n",
    "        x = token_feats.transpose(1, 2)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_collate(tokenizer, max_length: int):\n",
    "    def collate(batch):\n",
    "        pids, seqs, ys = zip(*batch)\n",
    "        tokens = tokenizer(\n",
    "            list(seqs),\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=True\n",
    "        )\n",
    "        ys = torch.stack(ys, dim=0)\n",
    "        return list(pids), tokens, ys\n",
    "    return collate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(backbone, head, loader, device, criterion, optimizer=None, threshold=0.5):\n",
    "    train_mode = optimizer is not None\n",
    "    head.train(train_mode)\n",
    "    if train_mode: backbone.eval()\n",
    "    total_loss = 0.0\n",
    "    preds_all, trues_all = [], []\n",
    "\n",
    "    for pids, tokens, ys in tqdm(loader, desc=\"train\" if train_mode else \"eval\", total=len(loader)):\n",
    "        ys = ys.to(device)\n",
    "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feats = backbone(**tokens).last_hidden_state\n",
    "\n",
    "        logits = head(feats)\n",
    "        loss = criterion(logits, ys)\n",
    "\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(head.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * ys.size(0)\n",
    "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        preds = (probs >= threshold).astype(np.int32)\n",
    "        preds_all.append(preds)\n",
    "        trues_all.append(ys.detach().cpu().numpy())\n",
    "\n",
    "    preds_all = np.vstack(preds_all)\n",
    "    trues_all = np.vstack(trues_all)\n",
    "    f1 = f1_score(trues_all, preds_all, average=\"micro\", zero_division=0)\n",
    "    return total_loss / len(loader.dataset), f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Submission Builder Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_submission(head, backbone, tokenizer, seq_dict: Dict[str, str], pids: List[str],\n",
    "                     mlb: MultiLabelBinarizer, device, max_length: int,\n",
    "                     out_path: Path, keep_top_per_protein: int = 1500, min_score: float = 0.01):\n",
    "    head.eval(); backbone.eval()\n",
    "    ds = ProteinDataset(pids, torch.zeros((len(pids), len(mlb.classes_)), dtype=torch.float32), seq_dict)\n",
    "    loader = DataLoader(ds, batch_size=8, shuffle=False, collate_fn=make_collate(tokenizer, max_length))\n",
    "\n",
    "    all_rows = []\n",
    "    with torch.no_grad():\n",
    "        for pids_b, tokens, _ in tqdm(loader, desc=\"predict\", total=len(loader)):\n",
    "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "            feats = backbone(**tokens).last_hidden_state\n",
    "            logits = head(feats)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            for pid, prob in zip(pids_b, probs):\n",
    "                go_scores = [(go, float(s)) for go, s in zip(mlb.classes_, prob) if s > min_score]\n",
    "                if not go_scores:\n",
    "                    continue\n",
    "                go_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                go_scores = go_scores[:keep_top_per_protein]\n",
    "                for go, s in go_scores:\n",
    "                    s = max(min(s, 1.0), 1e-6)\n",
    "                    all_rows.append((pid, go, s))\n",
    "\n",
    "    sub = pd.DataFrame(all_rows, columns=[\"protein_id\", \"go_term\", \"score\"])\n",
    "    sub = sub.sort_values([\"protein_id\", \"score\"], ascending=[True, False]) \\\n",
    "             .groupby(\"protein_id\").head(keep_top_per_protein)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    sub.to_csv(out_path, sep=\"\\t\", index=False, header=False)\n",
    "    print(f\"[OK] Submission written: {out_path}  (lines={len(sub)})\")\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Main Execution\n",
    "\n",
    "### 10.1 Initialize and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "# Check dependencies\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "except Exception:\n",
    "    fail(\"Missing dependency: transformers. Please `pip install transformers`.\")\n",
    "\n",
    "# Check base path\n",
    "base = Path(BASE_PATH)\n",
    "if not base.exists():\n",
    "    fail(f\"Base path not found: {base}\")\n",
    "\n",
    "print(\"[Info] Loading tables & FASTA paths ...\")\n",
    "train_terms, train_taxonomy, train_fasta, test_fasta = load_tables(base)\n",
    "if not train_fasta or not Path(train_fasta).exists():\n",
    "    fail(\"train_sequences.fasta not found.\")\n",
    "\n",
    "print(\"[Info] Loading sequences ...\")\n",
    "train_sequences = load_fasta_dict(train_fasta)\n",
    "test_sequences = load_fasta_dict(test_fasta) if test_fasta else {}\n",
    "\n",
    "print(f\"[Info] Loaded {len(train_sequences)} training sequences\")\n",
    "if test_sequences:\n",
    "    print(f\"[Info] Loaded {len(test_sequences)} test sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Prepare Labels and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[Info] Building label space (Top-K={TOP_K}) ...\")\n",
    "subset = train_terms.copy()\n",
    "top_terms = subset[\"go_term\"].value_counts().head(TOP_K).index\n",
    "subset = subset[subset[\"go_term\"].isin(top_terms)]\n",
    "labels_df = subset.groupby(\"protein_id\")[\"go_term\"].apply(list).reset_index()\n",
    "labels_df = labels_df[labels_df[\"protein_id\"].isin(train_sequences.keys())].reset_index(drop=True)\n",
    "\n",
    "if SAMPLE_N and SAMPLE_N > 0:\n",
    "    labels_df = labels_df.sample(n=min(SAMPLE_N, len(labels_df)), random_state=SEED).reset_index(drop=True)\n",
    "    print(f\"[Info] Using sample_n={len(labels_df)} proteins for a quick run.\")\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(labels_df[\"go_term\"].tolist()).astype(np.float32)\n",
    "pid_list = labels_df[\"protein_id\"].tolist()\n",
    "\n",
    "# Train/val split\n",
    "rng = np.random.default_rng(SEED)\n",
    "idx = np.arange(len(pid_list)); rng.shuffle(idx)\n",
    "split = int(0.8 * len(idx))\n",
    "train_idx, val_idx = idx[:split], idx[split:]\n",
    "\n",
    "pids_train = [pid_list[i] for i in train_idx]\n",
    "pids_val   = [pid_list[i] for i in val_idx]\n",
    "Y_train = torch.tensor(Y[train_idx], dtype=torch.float32)\n",
    "Y_val   = torch.tensor(Y[val_idx], dtype=torch.float32)\n",
    "\n",
    "print(f\"[Info] Train/Val sizes: {len(pids_train)} / {len(pids_val)} ; labels={len(mlb.classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Load Backbone Model (Frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[Info] Loading backbone: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "backbone = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Freeze backbone parameters\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "device = pick_device()\n",
    "print(f\"[Info] Device: {device}\")\n",
    "backbone.to(device).eval()\n",
    "hidden_dim = backbone.config.hidden_size\n",
    "print(f\"[Info] Backbone hidden dimension: {hidden_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 Create Model Head and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CNN head\n",
    "head = CNNHead(in_dim=hidden_dim, num_labels=len(mlb.classes_)).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(head.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "print(f\"[Info] Model initialized with {sum(p.numel() for p in head.parameters()):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5 Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ProteinDataset(pids_train, Y_train, train_sequences)\n",
    "val_ds   = ProteinDataset(pids_val,   Y_val,   train_sequences)\n",
    "collate  = make_collate(tokenizer, MAX_LENGTH)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "\n",
    "print(f\"[Info] Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Info] Starting training ... (showing tqdm per-batch ETA)\")\n",
    "best_f1, best_state = -1.0, None\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_f1 = run_epoch(backbone, head, train_loader, device, criterion, optimizer, THRESHOLD)\n",
    "    va_loss, va_f1 = run_epoch(backbone, head, val_loader,   device, criterion, None,      THRESHOLD)\n",
    "    print(f\"[Epoch {ep}] train_loss={tr_loss:.4f} train_f1={tr_f1:.4f} | val_loss={va_loss:.4f} val_f1={va_f1:.4f}\")\n",
    "    \n",
    "    if va_f1 > best_f1:\n",
    "        best_f1, best_state = va_f1, head.state_dict().copy()\n",
    "        print(f\"  -> New best validation F1: {best_f1:.4f}\")\n",
    "\n",
    "if best_state:\n",
    "    head.load_state_dict(best_state)\n",
    "    print(f\"\\n[Result] Best val micro-F1: {best_f1:.4f}\")\n",
    "else:\n",
    "    print(f\"\\n[Result] Final val micro-F1: {va_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.7 Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine submission target\n",
    "if PREDICT_SPLIT == \"trainval\":\n",
    "    target_pids = pids_train + pids_val\n",
    "    seq_source = train_sequences\n",
    "    print(f\"[Info] Generating predictions for train+val split ({len(target_pids)} proteins)\")\n",
    "else:\n",
    "    if not test_sequences:\n",
    "        fail(\"No test superset FASTA found. Use PREDICT_SPLIT='trainval' or provide Test/testsuperset.fasta.\")\n",
    "    target_pids = list(test_sequences.keys())\n",
    "    seq_source = test_sequences\n",
    "    print(f\"[Info] Generating predictions for test split ({len(target_pids)} proteins)\")\n",
    "\n",
    "out_path = Path(SUBMISSION_PATH)\n",
    "print(f\"[Info] Building submission for {len(target_pids)} proteins â†’ {out_path}\")\n",
    "submission = build_submission(\n",
    "    head=head,\n",
    "    backbone=backbone,\n",
    "    tokenizer=tokenizer,\n",
    "    seq_dict=seq_source,\n",
    "    pids=target_pids,\n",
    "    mlb=mlb,\n",
    "    device=device,\n",
    "    max_length=MAX_LENGTH,\n",
    "    out_path=out_path,\n",
    "    keep_top_per_protein=1500,\n",
    "    min_score=0.01\n",
    ")\n",
    "\n",
    "print(\"\\n[Info] Training and submission generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.8 Preview Submission (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of submission\n",
    "print(\"\\nSubmission preview:\")\n",
    "print(submission.head(20))\n",
    "print(f\"\\nTotal rows in submission: {len(submission)}\")\n",
    "print(f\"Unique proteins: {submission['protein_id'].nunique()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
